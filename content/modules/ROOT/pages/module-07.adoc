= Section 8 - Splunk

== Splunk Setup and Router Configuration

In this lab environment, we‚Äôve included Splunk to integrate with Event Driven Ansible (EDA). Splunk collects and indexes log files and other data from network sources in real-time. This indexed data is then stored in a way that allows for incredibly fast and flexible searching and analysis. From a splunk search for OSPF related events, we can create a splunk alert to send webhooks or kafka topics to EDA. 

Think of Splunk as an Observabiltiy tool. 

[quote]
üí° Observability tools, such as Splunk, Prometheus, Grafana, Dynatrace, and others, provide deep insights into the health and performance of IT systems. They excel at collecting metrics, logs, and traces to identify when something is wrong. However, they traditionally rely on human intervention to interpret the alerts and take corrective action. This is where Ansible EDA steps in, creating a powerful synergy.

=== Accessing Splunk
[cols="2,2", options="header"]
|===
| Component
| Value

| Splunk URL
| link:http://{target_host}:8000[Splunk,window=_blank]

| Username
| admin

| Password
| `{ssh_password}`
|===

	1.	Open Splunk using the above details ( use incognito window if needed )
	2.	Login using the credentials provided

=== Data Inputs
In Splunk, "data inputs" are the various methods and sources from which the platform can ingest data for indexing, searching, and analysis. They are the pathways that feed raw machine data into the Splunk system.
  	
Once logged in, click on the settings at the top of the screen and data inputs

image::data_inputs.png[Data Inputs, 100%]

==== TCP Port 5514
Splunk has been pre-loaded to open TCP port 5514. We simply need to add a data input to receive Cisco IOS syslogs using this port.

Step1:  Add a new TCP input


image::tcp_1.png[Add TCP, 100%]

Step2: Click on "New Local TCP"

image::tcp2.png[Add TCP, 100%]

Step3: Select TCP, port 5514, and click "next" 

image::tcp_3.png[Add TCP, 100%]

Step4: Configure the following input settings

=== Input Settings
[cols="2,2", options="header"]
|===
| Component
| Value

| Source Type
| cisco:ios

| App Context
| Cisco Networks (cisco:ios)

| Host 
| IP
|===

image::tcp_4.png[Add TCP, 100%]

Step5: Validate your cofiguration and submit

image::tcp_5.png[Add TCP, 100%]

=== Cisco Routers 

This lab includes two Cisco Cat8000v routers. Both routers use interface tunnel0 for the OPSF connection. 

==== Router setup playbook
This job template, "Network-Router-Setup," automates the initial configuration of network routers.

In addition to establishing OSPF routing, this playbook now configures the target device (e.g., cisco-rtr1) to send all syslog messages to Splunk over TCP port 5514 for centralized logging and monitoring.

Access AAP and run the job-template

image::router_setup.png[Router Setup, 100%]

=== Verify Syslogs
Step1: After the Cisco router setup cpmpletes, return to splunk and click on the Cisco Networks App

image::cisco_networks.png[Cisco networks]

Step2: Enter the routing dashboard

image::routing_dashboard1.png[Routing Dash]

Step3: Verify that there is an entry of an OSPF event with a full adjacency 

image::routing_dashboard2.png[Routing Dash]

=== Configure an Alert
In splunk we can create an alert from a search. A Splunk search is when you manually run a query to find and analyze information in your data.

An alert is simply that search saved to run automatically if the search results meet a trigger condition you set (like "%OSPF-5-ADJCHG: Process 1, Nbr 192.168.2.2 on Tunnel0 from FULL to DOWN"), Splunk automatically performs an action, such as sending a webhook to EDA.

Step1: Access the Splunk search field

image::search1.png[Search, 100%]

[quote]
üí° You will return to this browser tab after completing the next step

Step2: Create an OPSF neighbor down event
Access cisco-rtr1 from the bastion console to shut down interface tunnel0

image::bastion.png[bastion, 100%]

SSH pasword=`{ssh_password}` SSH into cisco-rtr1 and add the following commands
 
----
ssh admin@cisco-rtr1
config t
int tu 0
shut
----

Step3: Return to the Splunk search

paste in the following line and search: 
----
 %OSPF-5-ADJCHG: Process 1, Nbr 192.168.2.2 on Tunnel0 from FULL to DOWN
----

image::search2.png[Search]

Step4: Save as an Alert (ospf-neighbor)

image::save_as_alert.png[Save As]

Complete the pop-up screen for save as alert

=== Alert Settings
[cols="2,2", options="header"]
|===
| Component
| Value

| Source Type
| cisco:ios

| App Context
| Cisco Networks (cisco:ios)

| Host 
| IP
|===



image::save_as_alert2.png[Save As, 60%]







In the Town Square channel, you will eventually see:

	‚Ä¢	üîç Error logs from the Apache HTTPD service (collected by Filebeat and forwarded via Kafka)
	‚Ä¢	üß† AI Insights generated from the logs using RHEL AI

This real-time feed mimics how production environments might use automated ticket enrichment ‚Äî by capturing logs and insights and sending them directly to a ticketing system like ServiceNow.

[quote]
You won‚Äôt see any messages just yet ‚Äî this step is simply to explain where Mattermost fits in and why this channel matters. Logs and insights will start appearing once the first workflow runs.

[quote]
üí° With this integration, you are essentially watching AI and automation work together ‚Äî detecting the issue, diagnosing it, and preparing remediation guidance before anyone intervenes.


== Build the Workflow

[cols="2,2,2", options="header"]
|===
| System | URL | Credentials

| Ansible Automation Platform
| AAP is preloaded in the lab interface.
Click link if you want to open it in full tab:
https://{nginx_web_url}[AAP Web UI,window=_blank]
| Username: `{lab_username}`
Password: `{ssh_password}`

|===

1. Login to Ansible Automation Platform.
2. Go to Automation Execution ‚Üí Templates.

image::automation_execution_templates.png[automation_execution_templates]

[start=3]
3. Click Create template ‚Üí Create workflow job template.

image::create_workflow.png[create_workflow,300]

[start=4]
4. Fill in the details:

[options="header"]
|===
| Parameter | Value
| Name | AI Insights and Lightspeed prompt generation
| Organization | Default
|===

[start=5]
5. Click Create workflow job template.

image::create_workflow_job_template.png[create_workflow_job_template,300]

[start=6]
6. You‚Äôll see the empty workflow visualizer.

image::currently_no_nodes_workflow.png[currently_no_nodes_workflow,400]

[start=7]
7. Add the Apache Service Status Check node:

[options="header"]
|===
| Parameter | Value
| Node type | Job Template
| Job Template | ‚öôÔ∏è Apache Service Status Check
| Convergence | Any
| Node alias | (You can leave this blank)
|===

image::add_apache_status_check_step.png[Add Apache Status Check Step]

[start=8]
8. Click Next, then Finish.

image::blue_next_button.png[blue_next_button,150]
image::blue_finish_button.png[blue_finish_button,150]

[start=9]
9. Visual after first node:

image::workflow_after_apache_status_node.png[Workflow after Apache node]

[start=10]
10. Add RHEL AI: Analyze Incident step:

11. Click on the three dots (‚ãÆ kebab menu) next to the *‚öôÔ∏è Apache Service Status Check* node.

12. Click on *‚äï Add step and link* to insert the next node into the workflow.

image::workflow_add_step_and_link.png[workflow_add_step_and_link,200]

[options="header"]
|===
| Parameter | Value
| Node type | Job Template
| Job Template | ü§ñ RHEL AI: Analyze Incident
| Status | Run on success
| Convergence | Any
| Node alias | (You can leave this blank)
|===

image::add_rhel_ai_step.png[Add RHEL AI Step]

[start=13]
13. Click Next, then Finish.

image::blue_next_button.png[]
image::blue_finish_button.png[]

[start=14]
14. Workflow with two nodes:

image::workflow_after_rhel_ai_step.png[After RHEL AI step]

[start=15]
15. Add Notify via Mattermost:

16. Click on the three dots (‚ãÆ kebab menu) next to the *ü§ñ RHEL AI: Analyze Incident* node.

17. Click on *‚äï Add step and link* to insert the next node into the workflow.

image::workflow_add_step_and_link.png[workflow_add_step_and_link,200]

[options="header"]
|===
| Parameter | Value
| Node type | Job Template
| Job Template | üì£ Notify via Mattermost
| Status | Run on success
| Convergence | Any
| Node alias | (You can leave this blank)
|===

image::add_mattermost_step.png[Add Mattermost Step]

[start=18]
18. Click Next, then Finish.

image::workflow_after_mattermost_step.png[After Mattermost step]

[start=19]
19. Add Build Ansible Lightspeed Job Template:

20. Click on the three dots (‚ãÆ kebab menu) next to the *üì£ Notify via Mattermost* node.

21. Click on *‚äï Add step and link* to insert the next node into the workflow.

image::workflow_add_step_and_link.png[workflow_add_step_and_link,200]

[options="header"]
|===
| Parameter | Value
| Node type | Job Template
| Job Template | ‚öôÔ∏è Build Ansible Lightspeed Job Template
| Status | Run on success
| Convergence | Any
| Node alias | (You can leave this blank)
|===

image::add_lightspeed_jt_creator.png[Add Lightspeed JT Creator Step]

[start=21]
22. Click Next, then Finish.

image::blue_next_button.png[]
image::blue_finish_button.png[]

[start=23]
23. Final workflow visual:

image::workflow_final_prompt_generation.png[Final Workflow]

[start=24]
24. Click Save to finalize.

image::save_button.png[]

== Trigger the Workflow

[start=25]
25. Run the `‚ùå Break Apache` job template. This inserts an invalid directive in Apache config and restarts the service.

image::run_break_apache.png[]

[start=26]
26. Go to Automation Decisions(Event-Driven Ansible) ‚Üí Rulebook Activations. Confirm EDA(Event-Driven Ansible) picked up the event.

image::eda_trigger_capture.png[]

[start=27]
27. Go to Automation Controller ‚Üí Jobs. Confirm workflow execution. When the workflow completes you will see a green ‚úÖ Success.

image::workflow_triggered_jobs.png[]


[start=28]
28. Go to Templates and you should be able to see a new job template called "üß† Lightspeed Remediation Playbook Generator" generated. This step also creates a blank Workflow Job Template called "Remediation Workflow" that will be used in next challenge.

== Final Checks & Observations

Before you move on to the remediation phase, take a moment to explore the **Mattermost Town Square** channel.

Here‚Äôs what to look for:

- üõë *HTTPD Error Logs*: These logs were automatically collected from the webserver.
- üß† *AI Insights (RCA)*: Red Hat AI parsed the logs and generated a root cause analysis. These insights help you understand exactly **why** the failure occurred.

This is what real-world AIOps looks like! Imagine this in a production setting:

[quote]
üí° A ticket appears in your ITSM tool with:
- Attached logs
- Root Cause Analysis (RCA)
- A prompt ready for Lightspeed to generate the fix

And all of this *before anyone has even opened the ticket*.

We used **Mattermost** in this lab because it's a lightweight, open-source chat platform that‚Äôs easy to run per student. But the same flow would work with **ServiceNow**, **Jira**, or any ITSM of your choice.

[quote]
üí° Think of this as **automated ticket enrichment**. Logs and RCA can be pre-attached to incidents or alerts, helping teams respond faster and more effectively.

link:_images/mattermost_logs_and_rca_placeholder.png[image:mattermost_logs_and_rca_placeholder.png[Mattermost logs and RCA message preview], window=_blank]

üéØ In short: You now have an intelligent incident response pipeline that goes from *failure detection* to *context* to *fix suggestion* ‚Äî powered by Red Hat AI and Ansible Automation Platform.

== Summary

You created a workflow that:

* Uses logs for root cause analysis via Red Hat AI

* Notifies Mattermost

* Prepares a Lightspeed prompt for automated playbook generation

In the next step, we‚Äôll use that prompt to fix Apache automatically!

== Complete

You have completed this module. Move forward to the next one to use the created templates.
